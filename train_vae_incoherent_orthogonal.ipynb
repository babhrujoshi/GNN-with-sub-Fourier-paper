{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87388d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON: @save\n",
    "using BSON: @load\n",
    "using CSV\n",
    "using DataFrames: DataFrame\n",
    "using Flux\n",
    "using Flux: logitbinarycrossentropy, binarycrossentropy\n",
    "using Flux.Data: DataLoader\n",
    "using Flux: chunk\n",
    "using ImageFiltering\n",
    "using MLDatasets: FashionMNIST\n",
    "using ProgressMeter: Progress, next!\n",
    "using Random\n",
    "using Zygote\n",
    "using MLDatasets\n",
    "using Images\n",
    "using ImageIO\n",
    "using LinearAlgebra\n",
    "using FFTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fbf96e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_BCE_shifted (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function create_vae()\n",
    "    # Define the encoder and decoder networks\n",
    "    encoder_features = Chain(\n",
    "        Dense(784,500, relu),\n",
    "        Dense(500,500, relu)\n",
    "    )\n",
    "    encoder_μ = Chain(encoder_features, Dense(500, 20))\n",
    "    encoder_logvar = Chain(encoder_features, Dense(500, 20))\n",
    "\n",
    "    W1 = randn(500,20)\n",
    "    W2 = randn(500,500)\n",
    "    W3 = Matrix(I(500))\n",
    "    Q = randn(784,500)\n",
    "\n",
    "    return encoder_μ, encoder_logvar, W1, W2, W3, Q\n",
    "end\n",
    "\n",
    "function  logit_BCE(x̂,x) \n",
    "    return sum(@.(- x * log( sigmoid(x̂) + 1e-5) - (1-x)* log( 1 - sigmoid(x̂) + 1e-5))) / size(x,2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4fca59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function vae_loss(encoder_μ, encoder_logvar, W1, W2, W3, Q, x, β, λ, F)\n",
    "    batch_size = size(x)[end]\n",
    "    @assert batch_size != 0\n",
    "\n",
    "    # Forward propagate through mean encoder and std encoders\n",
    "    μ = encoder_μ(x)\n",
    "    logvar = encoder_logvar(x)\n",
    "    # Apply reparameterisation trick to sample latent\n",
    "    z = μ + randn(Float32, size(logvar)) .* exp.(0.5f0 * logvar)\n",
    "    # Reconstruct from latent sample\n",
    "    x̂ = sigmoid(Q*relu(W2*relu(W1*z)))\n",
    "\n",
    "    # loss_α(F,A) = maximum(sqrt.(sum((F*A).*(F*A), dims = 2))) + 100*norm(A'*A - I(500),2)^2\n",
    "    # α = loss_α(F, Q) \n",
    "\n",
    "    # Negative reconstruction loss Ε_q[logp_x_z]\n",
    "    logp_x_z = sum(binarycrossentropy.(x̂, x))\n",
    "    # KL(qᵩ(z|x)||p(z)) where p(z)=N(0,1) and qᵩ(z|x) models the encoder i.e. reverse KL\n",
    "    # The @. macro makes sure that all operates are elementwise\n",
    "    kl_q_p = 0.5f0 * sum(@. (exp(logvar) + μ^2 - logvar - 1f0)) \n",
    "    # Weight decay regularisation term\n",
    "    reg = λ * sum(x->sum(x.^2), Flux.params(encoder_μ, encoder_logvar, W1, W2, Q))\n",
    "    # We want to maximise the evidence lower bound (ELBO)\n",
    "    elbo = - β .* kl_q_p\n",
    "    # So we minimise the sum of the negative ELBO and a weight penalty\n",
    "    return -elbo + reg + logp_x_z \n",
    "end\n",
    "\n",
    "\n",
    "function train(encoder_μ, encoder_logvar, W1, W2, W3, Q, dataloader, num_epochs, λ, β, optimiser, save_dir)\n",
    "    # The training loop for the model\n",
    "    trainable_params = Flux.params(encoder_μ, encoder_logvar, W1, W2, W3, Q)\n",
    "    progress_tracker = Progress(num_epochs, \"Training a epoch done\")\n",
    "    F = dct(diagm(ones(784)),2);\n",
    "\n",
    "    for epoch_num = 21:num_epochs\n",
    "        acc_loss = 0.0\n",
    "        loss = 0\n",
    "        for (x_batch, y_batch) in dataloader\n",
    "            # pullback function returns the result (loss) and a pullback operator (back)\n",
    "            loss, back = pullback(trainable_params) do\n",
    "                vae_loss(encoder_μ, encoder_logvar, W1, W2, W3, Q, x_batch, β, λ, F)\n",
    "            end\n",
    "            # Feed the pullback 1 to obtain the gradients and update then model parameters\n",
    "            gradients = back(1f0)\n",
    "            Flux.Optimise.update!(optimiser, trainable_params, gradients)\n",
    "            if isnan(loss)\n",
    "                break\n",
    "            end\n",
    "            acc_loss += loss\n",
    "        end\n",
    "        next!(progress_tracker; showvalues=[(:loss, loss)])\n",
    "        @assert length(dataloader) > 0\n",
    "        avg_loss = acc_loss / length(dataloader)\n",
    "        metrics = DataFrame(epoch=epoch_num, negative_elbo=avg_loss)\n",
    "        # println(metrics)\n",
    "        CSV.write(joinpath(save_dir, \"metrics.csv\"), metrics, header=(epoch_num==1), append=true)\n",
    "        save_model(encoder_μ, encoder_logvar, W1, W2, W3, Q, save_dir, epoch_num)\n",
    "    end\n",
    "    println(\"Training complete!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "339ecd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.traindata() is deprecated, use `MNIST(split=:train)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Babhru\\.julia\\packages\\MLDatasets\\Xb4Lh\\src\\datasets\\vision\\mnist.jl:187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining a epoch done   5%|██                            |  ETA: 0:18:52\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6676.014179115087\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done   8%|███                           |  ETA: 0:18:12\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6340.157361660914\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  10%|████                          |  ETA: 0:17:37\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6725.146820701426\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  12%|████                          |  ETA: 0:17:04\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6979.411996245366\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  15%|█████                         |  ETA: 0:16:32\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6986.1559459612845\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  18%|██████                        |  ETA: 0:16:01\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6529.688310352918\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  20%|███████                       |  ETA: 0:15:30\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6933.335847306184\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  22%|███████                       |  ETA: 0:15:00\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6938.816460170531\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  25%|████████                      |  ETA: 0:14:29\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6400.1495335902\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  28%|█████████                     |  ETA: 0:14:00\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6768.413960781283\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  30%|██████████                    |  ETA: 0:13:31\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6626.962016120346\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  32%|██████████                    |  ETA: 0:13:01\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6490.669199879254\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  35%|███████████                   |  ETA: 0:12:31\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  7011.768501165402\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  38%|████████████                  |  ETA: 0:12:02\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6669.3725730919\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  40%|█████████████                 |  ETA: 0:11:33\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6836.976610947645\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  42%|█████████████                 |  ETA: 0:11:03\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  7015.519353430349\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  45%|██████████████                |  ETA: 0:10:34\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6959.791039211978\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  48%|███████████████               |  ETA: 0:10:05\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6743.317300703807\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...Done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r\u001b[K\u001b[A\r\u001b[32mTraining a epoch done  50%|████████████████              |  ETA: 0:09:36\u001b[39m\u001b[K\r\n",
      "\u001b[34m  loss:  6904.55764469664\u001b[39m\u001b[K\r\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...Done\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "shuffle_data = true\n",
    "η = 0.001\n",
    "β = 1f0\n",
    "λ = 0.01f0\n",
    "num_epochs = 40\n",
    "\n",
    "save_dir = \"trained_GNN/MNIST_sigmoid_v2\"\n",
    "# Define the model and create our data loader\n",
    "dataloader = get_train_loader(batch_size, shuffle_data)\n",
    "# encoder_μ, encoder_logvar, W1, W2, W3, Q  = create_vae()\n",
    "train(encoder_μ, encoder_logvar, W1, W2, W3, Q, dataloader, num_epochs, λ, β, ADAM(η, (0.9,0.8)), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26330dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "visualise (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function reconstruct_images(encoder_μ, encoder_logvar, W1, W2, W3, Q, x)\n",
    "    # Forward propagate through mean encoder and std encoders\n",
    "    μ = encoder_μ(x)\n",
    "    logvar = encoder_logvar(x)\n",
    "    # Apply reparameterisation trick to sample latent\n",
    "    z = μ + randn(Float32, size(logvar)) .* exp.(0.5f0 * logvar)\n",
    "    # Reconstruct from latent sample\n",
    "    x̂ = sigmoid(Q*relu(W2*relu(W1*z)))\n",
    "    return clamp.(x̂, 0 ,1)\n",
    "end\n",
    "\n",
    "function visualise()\n",
    "    # Define some parameters\n",
    "    batch_size = 64\n",
    "    shuffle = true\n",
    "    num_images = 30\n",
    "    epoch_to_load = 20\n",
    "    # Load the model and test set loader\n",
    "    dir = \"trained_GNN/MNIST_sigmoid_v2\"\n",
    "    encoder_μ, encoder_logvar, W1, W2, W3, Q = load_model_sep(dir, epoch_to_load)\n",
    "    dataloader = get_test_loader(batch_size, shuffle)\n",
    "    # Reconstruct and save some images\n",
    "    for (x_batch, y_batch) in dataloader\n",
    "        save_to_images(x_batch, dir, \"test-image\", num_images)\n",
    "        x̂_batch = reconstruct_images(encoder_μ, encoder_logvar, W1, W2, W3, Q, x_batch)\n",
    "        save_to_images(x̂_batch, dir, \"reconstruction\", num_images)\n",
    "        break\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaf26eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: MNIST.testdata() is deprecated, use `MNIST(split=:test)[:]` instead.\n",
      "└ @ MLDatasets C:\\Users\\Babhru\\.julia\\packages\\MLDatasets\\Xb4Lh\\src\\datasets\\vision\\mnist.jl:195\n"
     ]
    }
   ],
   "source": [
    "visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c4dff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAqlJREFUaAW9wb9r3HUcB+Dnc/fKL03VUgMBRVtCnQoOKjiJSEEcBDf/Ev8CF0dHFwUHwanirKWgm1JCCw7+KBQrWshhNRhjkrv7OtwgIbkfKfJ+nnRqRbEoFsWiWBSLYlEsHkJnojm7KBbFoljM0Jno8Ddu4zpu4le8gHdwET2LiWJRLIrFDA1j7GMbn+BrDNCwgw7v4XE080WxKBbFYo4xdnAD29jBHkbYxXV8i9fQN18Ui2JRLGbocIQHuIMBRhhjhCF+wbt4GefMF8WiWBSLOfro4zw2MUQfezjCEHfxG9bRzBbFolgUixkaeljHVTyL+/gJ27iPI+xhB5fNF8WiWBSLOfrYxBqewi0s4Q52MMYIBxYTxaJYFIs5GlbRRx8H6LCBexiihwE6NLNFsSgWxWIBPSzjCWxhAz/jB/yDQ3yDN7FmtigWxaJYLKhhCRewjssmRmgYYIQOzXRRLIpFsTiDhg6H+BK7GGEZWxYTxaJYFIszGGMf13ATByZWcQk980WxKBbFYgEdRriHj/ApdtBhGVt4BjHRmWhOimJRLIrFDB0OcRc3cA3fYRcjdDiP17GOMTo000WxKBbFYooOv+N9fIw/cWRiGUs4RDDA9+iwgQtYQR/NcVEsikWxmOIAn+ED/IEOfawhGGOIAT7HbVzBK3geF/EImuOiWBSLYnGKDn/hK+xihIYx9tFhjA5HOMQeHqCHTVxyuigWxaJYTDEy0dDQYYyx4/poaFjCFp7DMpqTolgUi2JxiobH8Da28SNGaCaWsIINbCA4hzfwFp5E0JwUxaJYFIspVvEqPsQXuIUDnMNVXMHTWMEQwSpW0DNdFItiUSymaHgUL+FF/2loHl4Ui2JRLOZoaP4/USyKRbEoFsWiWBSLYlEsikWxKBbFolgU+xdy0pDb2IFNlQAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAqlJREFUaAW9wb9r3HUcB+Dnc/fKL03VUgMBRVtCnQoOKjiJSEEcBDf/Ev8CF0dHFwUHwanirKWgm1JCCw7+KBQrWshhNRhjkrv7OtwgIbkfKfJ+nnRqRbEoFsWiWBSLYlEsHkJnojm7KBbFoljM0Jno8Ddu4zpu4le8gHdwET2LiWJRLIrFDA1j7GMbn+BrDNCwgw7v4XE080WxKBbFYo4xdnAD29jBHkbYxXV8i9fQN18Ui2JRLGbocIQHuIMBRhhjhCF+wbt4GefMF8WiWBSLOfro4zw2MUQfezjCEHfxG9bRzBbFolgUixkaeljHVTyL+/gJ27iPI+xhB5fNF8WiWBSLOfrYxBqewi0s4Q52MMYIBxYTxaJYFIs5GlbRRx8H6LCBexiihwE6NLNFsSgWxWIBPSzjCWxhAz/jB/yDQ3yDN7FmtigWxaJYLKhhCRewjssmRmgYYIQOzXRRLIpFsTiDhg6H+BK7GGEZWxYTxaJYFIszGGMf13ATByZWcQk980WxKBbFYgEdRriHj/ApdtBhGVt4BjHRmWhOimJRLIrFDB0OcRc3cA3fYRcjdDiP17GOMTo000WxKBbFYooOv+N9fIw/cWRiGUs4RDDA9+iwgQtYQR/NcVEsikWxmOIAn+ED/IEOfawhGGOIAT7HbVzBK3geF/EImuOiWBSLYnGKDn/hK+xihIYx9tFhjA5HOMQeHqCHTVxyuigWxaJYTDEy0dDQYYyx4/poaFjCFp7DMpqTolgUi2JxiobH8Da28SNGaCaWsIINbCA4hzfwFp5E0JwUxaJYFIspVvEqPsQXuIUDnMNVXMHTWMEQwSpW0DNdFItiUSymaHgUL+FF/2loHl4Ui2JRLOZoaP4/USyKRbEoFsWiWBSLYlEsikWxKBbFolgU+xdy0pDb2IFNlQAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{N0f8}:\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)  …  Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)\n",
       " Gray{N0f8}(1.0)  Gray{N0f8}(1.0)     Gray{N0f8}(1.0)  Gray{N0f8}(1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"trained_GNN/MNIST_sigmoid_inco/reconstruction-3.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
